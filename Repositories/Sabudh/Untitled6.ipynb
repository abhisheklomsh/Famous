{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Momentum----------------------------------------------------1\n",
      "main----------------------------------------------------1\n",
      "pm.Model----------------------------------------------------1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhisheklomsh/anaconda/lib/python2.7/site-packages/pymc3/model.py:1266: UserWarning: Data in r contains missing values and will be automatically imputed from the sampling distribution.\n",
      "  warnings.warn(impute_message, UserWarning)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp500_model----------------------------------------------------1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [r_missing, s, sigma, nu]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"****************************\n",
    "Pairs Trading Strategy\n",
    "****************************\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from zipline.utils import tradingcalendar\n",
    "import pytz\n",
    "#from mpl_finance import candlestick_ohlc as quotes_historical_yahoo #\n",
    "from yahoo_finance import Share\n",
    "\n",
    "def initialize(context):\n",
    "    print(\"initialize----------------------------------------------------1\")\n",
    "    # Quantopian backtester specific variables\n",
    "    set_symbol_lookup_date('2014-01-01')\n",
    "\n",
    "    set_slippage(slippage.VolumeShareSlippage(volume_limit=0.025, price_impact=0.1))\n",
    "    set_commission(commission.PerShare(cost=0.0075, min_trade_cost=1))                  \n",
    "            \n",
    "    context.stock_pairs = [(symbol('ABGB'), symbol('FSLR')),\n",
    "                           (symbol('CSUN'), symbol('ASTI')),\n",
    "                           (symbol('KO'),   symbol('PEP')),\n",
    "                           (symbol('AAPL'), symbol('IBM')),\n",
    "                           (symbol('FB'),   symbol('YHOO')),\n",
    "                           (symbol('TWTR'), symbol('YHOO'))]    \n",
    "    \n",
    "    context.stocks = [symbol('ABGB'), symbol('FSLR'), symbol('CSUN'), symbol('ASTI'),\\\n",
    "                      symbol('KO'), symbol('PEP'), symbol('AAPL'), symbol('IBM'), symbol('FB'),\\\n",
    "                      symbol('YHOO'),symbol('TWTR')]\n",
    "    \n",
    "    context.num_pairs = len(context.stock_pairs)\n",
    "    # strategy specific variables\n",
    "    context.lookback = 20 # used for regression\n",
    "    context.z_window = 20 # used for zscore calculation, must be <= lookback\n",
    "    \n",
    "    context.spread = np.ndarray((context.num_pairs, 0))\n",
    "    # context.hedgeRatioTS = np.ndarray((context.num_pairs, 0))\n",
    "    context.inLong = [False] * context.num_pairs\n",
    "    context.inShort = [False] * context.num_pairs\n",
    "    \n",
    "    schedule_function(func=check_pair_status, date_rule=date_rules.every_day(),\\\n",
    "                      time_rule=time_rules.market_close(minutes=90))   \n",
    "\n",
    "def check_pair_status(context, data):\n",
    "    print(\"check_pair_status----------------------------------------------------1\")\n",
    "    if get_open_orders():\n",
    "        return    \n",
    "\n",
    "    prices = data.history(context.stocks, fields='price', bar_count=35, frequency='1d').\\\n",
    "             iloc[-context.lookback::]\n",
    "    \n",
    "    new_spreads = np.ndarray((context.num_pairs, 1))\n",
    "    \n",
    "    for i in range(context.num_pairs):\n",
    "\n",
    "        (stock_y, stock_x) = context.stock_pairs[i]\n",
    "\n",
    "        Y = prices[stock_y]\n",
    "        X = prices[stock_x]\n",
    "\n",
    "        try:\n",
    "            hedge = hedge_ratio(Y, X, add_const=True)   \n",
    "            record(hedge=hedge)\n",
    "        except ValueError as e:\n",
    "            log.debug(e)\n",
    "            return\n",
    "\n",
    "        # context.hedgeRatioTS = np.append(context.hedgeRatioTS, hedge)\n",
    "        \n",
    "        new_spreads[i, :] = Y[-1] - hedge * X[-1]\n",
    "\n",
    "        if context.spread.shape[1] > context.z_window:\n",
    "            # Keep only the z-score lookback period\n",
    "            spreads = context.spread[i, -context.z_window:]\n",
    "\n",
    "            zscore = (spreads[-1] - spreads.mean()) / spreads.std()\n",
    "            record(zscore=zscore)\n",
    "            \n",
    "            if context.inShort[i] and zscore < 0.0:\n",
    "                order_target(stock_y, 0)\n",
    "                order_target(stock_x, 0)\n",
    "                context.inShort[i] = False\n",
    "                context.inLong[i] = False\n",
    "                record(X_pct=0, Y_pct=0)\n",
    "                return\n",
    "\n",
    "            if context.inLong[i] and zscore > 0.0:\n",
    "                order_target(stock_y, 0)\n",
    "                order_target(stock_x, 0)\n",
    "                context.inShort[i] = False\n",
    "                context.inLong[i] = False\n",
    "                record(X_pct=0, Y_pct=0)\n",
    "                return\n",
    "\n",
    "            if zscore < -1.0 and (not context.inLong[i]):\n",
    "                # Only trade if NOT already in a trade\n",
    "                y_target_shares = 1       #long y\n",
    "                X_target_shares = -hedge  #short x\n",
    "                context.inLong[i] = True\n",
    "                context.inShort[i] = False\n",
    "\n",
    "                (y_target_pct, x_target_pct) = computeHoldingsPct( y_target_shares,X_target_shares, Y[-1], X[-1] )\n",
    "                order_target_percent( stock_y, y_target_pct * (1.0/context.num_pairs) )\n",
    "                order_target_percent( stock_x, x_target_pct * (1.0/context.num_pairs) )\n",
    "                record(Y_pct=y_target_pct, X_pct=x_target_pct)\n",
    "                return\n",
    "\n",
    "            if zscore > 1.0 and (not context.inShort[i]):\n",
    "                # Only trade if NOT already in a trade\n",
    "                y_target_shares = -1     #short y\n",
    "                X_target_shares = hedge  #long x\n",
    "                context.inShort[i] = True\n",
    "                context.inLong[i] = False\n",
    "\n",
    "                (y_target_pct, x_target_pct) = computeHoldingsPct( y_target_shares, X_target_shares, Y[-1], X[-1] )\n",
    "                order_target_percent( stock_y, y_target_pct * (1.0/context.num_pairs))\n",
    "                order_target_percent( stock_x, x_target_pct * (1.0/context.num_pairs))\n",
    "                record(Y_pct=y_target_pct, X_pct=x_target_pct)\n",
    "        \n",
    "    context.spread = np.hstack([context.spread, new_spreads])\n",
    "\n",
    "def hedge_ratio(Y, X, add_const=True):\n",
    "    print(\"hedge_ratio----------------------------------------------------1\")\n",
    "    if add_const:\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(Y, X).fit()\n",
    "        return model.params[1]\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    return model.params.values\n",
    "    \n",
    "def computeHoldingsPct(yShares, xShares, yPrice, xPrice):\n",
    "    print(\"computeHoldingsPct----------------------------------------------------1\")\n",
    "    yDol = yShares * yPrice\n",
    "    xDol = xShares * xPrice\n",
    "    notionalDol =  abs(yDol) + abs(xDol)\n",
    "    y_target_pct = yDol / notionalDol\n",
    "    x_target_pct = xDol / notionalDol\n",
    "    return (y_target_pct, x_target_pct)\n",
    "\n",
    "def handle_data(context, data):\n",
    "    print(\"handle_data----------------------------------------------------1\")\n",
    "    pass\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\"\"\"****************************\n",
    "Long Short Strategy\n",
    "****************************\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from zipline.api import attach_pipeline, pipeline_output\n",
    "import zipline.pipeline.pipeline as Pipeline #from quantopian.pipeline import Pipeline\n",
    "from zipline.pipeline.factors.basic  import CustomFactor, SimpleMovingAverage, AverageDollarVolume\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "#from quantopian.pipeline.data import morningstar\n",
    "\n",
    "import zipline.pipeline.filters.filter#from quantopian.pipeline.filters import Q1500US\n",
    "\n",
    "# Constraint Parameters\n",
    "NUM_LONG_POSITIONS = 5\n",
    "NUM_SHORT_POSITIONS = 5\n",
    "\n",
    "class Momentum(CustomFactor):\n",
    "    print(\"Momentum----------------------------------------------------1\")\n",
    "\n",
    "    inputs = [USEquityPricing.close]\n",
    "    window_length = 252\n",
    "\n",
    "    def compute(self, today, assets, out, prices):\n",
    "        out[:] = ((prices[-21] - prices[-252])/prices[-252] -\n",
    "                  (prices[-1] - prices[-21])/prices[-21])\n",
    "\n",
    "def make_pipeline():\n",
    "    print(\"make_pipeline----------------------------------------------------1\")\n",
    "    \n",
    "    # define alpha factors\n",
    "    momentum = Momentum()\n",
    "    growth = morningstar.operation_ratios.revenue_growth.latest\n",
    "    pe_ratio = morningstar.valuation_ratios.pe_ratio.latest\n",
    "        \n",
    "    # Screen out non-desirable securities by defining our universe. \n",
    "    mkt_cap_filter = morningstar.valuation.market_cap.latest >= 500000000    \n",
    "    price_filter = USEquityPricing.close.latest >= 5\n",
    "    universe = Q1500US() & price_filter & mkt_cap_filter & \\\n",
    "               momentum.notnull() & growth.notnull() & pe_ratio.notnull()\n",
    "\n",
    "    combined_rank = (\n",
    "        momentum.rank(mask=universe).zscore() +\n",
    "        growth.rank(mask=universe).zscore() +\n",
    "        pe_ratio.rank(mask=universe).zscore()\n",
    "    )\n",
    "\n",
    "    longs = combined_rank.top(NUM_LONG_POSITIONS)\n",
    "    shorts = combined_rank.bottom(NUM_SHORT_POSITIONS)\n",
    "\n",
    "    long_short_screen = (longs | shorts)        \n",
    "\n",
    "    # Create pipeline\n",
    "    pipe = Pipeline(columns = {\n",
    "        'longs':longs,\n",
    "        'shorts':shorts,\n",
    "        'combined_rank':combined_rank,\n",
    "        'momentum':momentum,\n",
    "        'growth':growth,            \n",
    "        'pe_ratio':pe_ratio\n",
    "    },\n",
    "    screen = long_short_screen)\n",
    "    return pipe\n",
    "\n",
    "def initialize(context):\n",
    "    print(\"initialize----------------------------------------------------1\")\n",
    "\n",
    "    set_slippage(slippage.VolumeShareSlippage(volume_limit=0.025, price_impact=0.1))\n",
    "    set_commission(commission.PerShare(cost=0.0075, min_trade_cost=1))        \n",
    "\n",
    "    attach_pipeline(make_pipeline(), 'long_short_factors')\n",
    "\n",
    "    # Schedule my rebalance function\n",
    "    schedule_function(func=rebalance,\n",
    "                      date_rule=date_rules.month_start(),\n",
    "                      time_rule=time_rules.market_open(hours=1,minutes=30),\n",
    "                      half_days=True)\n",
    "    \n",
    "    # record my portfolio variables at the end of day\n",
    "    schedule_function(func=recording_statements,\n",
    "                      date_rule=date_rules.every_day(),\n",
    "                      time_rule=time_rules.market_close(),\n",
    "                      half_days=True)\n",
    "\n",
    "def before_trading_start(context, data):\n",
    "    print(\"before_trading_start----------------------------------------------------1\")\n",
    "    # Call pipeline_output to get the output\n",
    "    context.output = pipeline_output('long_short_factors')\n",
    "    \n",
    "    context.longs = context.output[context.output['longs']].index.tolist()\n",
    "    context.shorts = context.output[context.output['shorts']].index.tolist()\n",
    "\n",
    "    context.long_weight, context.short_weight = assign_weights(context)\n",
    "    \n",
    "    # These are the securities that we are interested in trading each day.\n",
    "    context.security_list = context.output.index\n",
    "   \n",
    "\n",
    "def assign_weights(context):\n",
    "    print(\"assign_weights----------------------------------------------------1\")\n",
    "    \"\"\"\n",
    "    Assign weights to securities that we want to order.\n",
    "    \"\"\"\n",
    "    long_weight = 0.5 / len(context.longs)\n",
    "    short_weight = -0.5 / len(context.shorts)\n",
    "        \n",
    "    return long_weight, short_weight\n",
    " \n",
    "def rebalance(context, data):\n",
    "    print(\"rebalance----------------------------------------------------1\")\n",
    "    \n",
    "    for security in context.portfolio.positions:\n",
    "        if security not in context.longs and \\\n",
    "        security not in context.shorts and data.can_trade(security):\n",
    "            order_target_percent(security, 0)\n",
    "\n",
    "    for security in context.longs:\n",
    "        if data.can_trade(security):\n",
    "            order_target_percent(security, context.long_weight)\n",
    "\n",
    "    for security in context.shorts:\n",
    "        if data.can_trade(security):\n",
    "            order_target_percent(security, context.short_weight)        \n",
    "    \n",
    "def recording_statements(context, data):\n",
    "    print(\"recording_statements----------------------------------------------------1\")\n",
    "    # Check how many long and short positions we have.\n",
    "    longs = shorts = 0\n",
    "    for position in context.portfolio.positions.itervalues():\n",
    "        if position.amount > 0:\n",
    "            longs += 1\n",
    "        elif position.amount < 0:\n",
    "            shorts += 1\n",
    "\n",
    "    # Record our variables.\n",
    "    record(leverage=context.account.leverage, long_count=longs, short_count=shorts)\n",
    "\t\n",
    "\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\"\"\"****************************\n",
    "Stochastic Volatility\n",
    "****************************\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas_datareader import data\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def main():\n",
    "    print(\"main----------------------------------------------------1\")\n",
    "\n",
    "    #load data    \n",
    "    returns = data.get_data_yahoo('SPY', start='2008-5-1', end='2009-12-1')['Close'].pct_change()\n",
    "    returns.plot()\n",
    "    plt.ylabel('daily returns in %');\n",
    "    \n",
    "    with pm.Model() as sp500_model:\n",
    "        print(\"pm.Model----------------------------------------------------1\")\n",
    "        \n",
    "        nu = pm.Exponential('nu', 1./10, testval=5.0)\n",
    "        sigma = pm.Exponential('sigma', 1./0.02, testval=0.1)\n",
    "        \n",
    "        s = pm.GaussianRandomWalk('s', sigma**-2, shape=len(returns))                \n",
    "        r = pm.StudentT('r', nu, lam=pm.math.exp(-2*s), observed=returns)\n",
    "        \n",
    "    \n",
    "    with sp500_model:\n",
    "        print(\"sp500_model----------------------------------------------------1\")\n",
    "        trace = pm.sample(2000)\n",
    "\n",
    "    pm.traceplot(trace, [nu, sigma]);\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    returns.plot()\n",
    "    plt.plot(returns.index, np.exp(trace['s',::5].T), 'r', alpha=.03)\n",
    "    plt.legend(['S&P500', 'stochastic volatility process'])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\n",
    "\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\"\"\"****************************\n",
    "Recurrent Neural Network\n",
    "****************************\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from pandas_datareader import data\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    print(\"create_dataset----------------------------------------------------1\")\n",
    "\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        dataX.append(dataset[i:(i+look_back),0])\n",
    "        dataY.append(dataset[i+look_back,0])\n",
    "\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #load data\n",
    "    start = datetime(2015, 1, 1, 0, 0, 0, 0, pytz.utc)\n",
    "    end = datetime(2016, 1, 1, 0, 0, 0, 0, pytz.utc)\n",
    "    spy = data.DataReader(\"SPY\", \"yahoo\", start, end)\n",
    "    dataset = np.array(spy['Close'].values).reshape(-1,1)\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "    # reshape for look_back\n",
    "    look_back = 10\n",
    "    X_train, y_train = create_dataset(train, look_back)\n",
    "    X_test, y_test = create_dataset(test, look_back)\n",
    "\n",
    "    # reshape for LSTM [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_dim=1)) #look_back))\n",
    "    model.add(Dense(1))\n",
    "    print(str(X_train.shape)+\"1\")\n",
    "    print(str(y_train.shape)+\"2\")\n",
    "    print(str(X_test.shape)+\"3\")\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(X_train, y_train, nb_epoch=100, batch_size=5, verbose=2)\n",
    "\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test) \n",
    "   \n",
    "    # scale back \n",
    "    train_pred = scaler.inverse_transform(train_pred)\n",
    "    #y_train = scaler.inverse_transform(y_train)\n",
    "    test_pred = scaler.inverse_transform(test_pred)\n",
    "    #y_test = scaler.inverse_transform(y_test)\n",
    "    print(\"Yes\")\n",
    "    # shift predictions for plotting\n",
    "    train_pred_plot = np.empty_like(dataset)\n",
    "    train_pred_plot[:,:] = np.nan\n",
    "    train_pred_plot[look_back:len(train_pred)+look_back,:] = train_pred\n",
    "\n",
    "    test_pred_plot = np.empty_like(dataset)\n",
    "    test_pred_plot[:,:] = np.nan\n",
    "    test_pred_plot[len(train_pred)+(look_back*2)+1:len(dataset)-1,:] = test_pred\n",
    "    print(\"Yes2\")\n",
    "    f = plt.figure()\n",
    "    plt.plot(scaler.inverse_transform(dataset), color='b', lw=2.0, label='S&P 500')\n",
    "    plt.plot(train_pred_plot, color='g', lw=2.0, label='LSTM train')\n",
    "    plt.plot(test_pred_plot, color='r', lw=2.0, label='LSTM test')\n",
    "    plt.legend(loc=3)\n",
    "    plt.grid(True)\n",
    "    f.savefig('./lstm.png')\n",
    "\t\n",
    "\t\n",
    "\n",
    "\t\n",
    "\"\"\"****************************\n",
    "Stock Clusters\n",
    "****************************\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "from sklearn.covariance import GraphicalLassoCV, ledoit_wolf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import cluster, manifold\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import quandl\n",
    "#from mpl_finance import \n",
    "#from matplotlib.finance import quotes_historical_yahoo_ochl as quotes_historical_yahoo\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"475 main----------------------------------------------------1\")\n",
    "    num_samples = 60\n",
    "    num_features = 20\n",
    "    \n",
    "    #generate data (synthetic)\n",
    "    #prec = make_sparse_spd_matrix(num_features, alpha=0.95, smallest_coef=0.4, largest_coef=0.7)\n",
    "    #cov = linalg.inv(prec)\n",
    "    #X = np.random.multivariate_normal(np.zeros(num_features), cov, size=num_samples)\n",
    "    #X = StandardScaler().fit_transform(X)    \n",
    "   \n",
    "    #generate data (actual)\n",
    "    STOCKS = {\n",
    "        'SPY': 'S&P500',\n",
    "        'LQD': 'Bond_Corp',\n",
    "        'TIP': 'Bond_Treas',\n",
    "        'GLD': 'Gold',\n",
    "        'MSFT': 'Microsoft',\n",
    "        'XOM':  'Exxon',\n",
    "        'AMZN': 'Amazon',\n",
    "        'BAC':  'BofA',\n",
    "        'NVS':  'Novartis'}\n",
    "      \n",
    "    symbols, names = np.array(list(STOCKS.items())).T\n",
    "   \n",
    "    start = datetime(2014, 1, 1, 0, 0, 0, 0, pytz.utc)\n",
    "    end = datetime(2016, 1, 1, 0, 0, 0, 0, pytz.utc)    \n",
    "\n",
    "    \"\"\"quotes = [quotes_historical_yahoo(symbol, start, end, asobject=True) for symbol in symbols]\"\"\"\n",
    "    \"\"\"yahoo = Share('YHOO')\n",
    "    print yahoo.get_open()\n",
    "    '36.60'\n",
    "    print yahoo.get_price()\n",
    "    '36.84'\n",
    "    \"\"\"\n",
    "    #mydata = quandl.get(\"FRED/GDP\", start_date=\"2001-12-25\", end_date=\"2005-12-31\")\n",
    "    #mydata = quandl.get(\"BATS/BATS_\"+symbol, start_date=\"2011-12-25\", end_date=\"2015-12-31\")\n",
    "\n",
    "    #quotes = [Share(symbol).get_historical(start, end) for symbol in symbols]\n",
    "    #quotes = [quandl.get(\"BATS/BATS_\"+symbol, start_date=start, end_date=end,open) for symbol in symbols]\n",
    "    quandl.ApiConfig.api_key = \"2bqtLaCsSnsM2XR3tCxU\"\n",
    "    \"\"\"for symbol in sum\n",
    "    data = quandl.get_table(\n",
    "                'WIKI/PRICES', qopts = { 'columns': ['ticker', 'date', 'open'] },\n",
    "                ticker = ['AAPL', 'MSFT'], date = { 'gte': '2016-01-01', 'lte': '2016-12-31' })\n",
    "    print(data['open'])\"\"\"\n",
    "        \n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "\n",
    "\n",
    "    data = quandl.get_table(\n",
    "                        'WIKI/PRICES', qopts = { 'columns': ['ticker', 'date', 'open','close'] },\n",
    "                        ticker = [symbols], date = { 'gte': start, 'lte': end })\n",
    "    #print(data.open.iloc[1:504])\n",
    "    \n",
    "    data_open = pd.concat([data.open.iloc[:504].reset_index(),\n",
    "                           data.open.iloc[504:1008].reset_index(),\n",
    "                           data.open.iloc[1008:1512].reset_index(),\n",
    "                           data.open.iloc[1512:2016].reset_index()],axis=1)\n",
    "    \n",
    "    \n",
    "    data_open = data_open.drop(\"None\",axis=1)\n",
    "    data_close = pd.concat([data.close.iloc[:504].reset_index(),\n",
    "                           data.close.iloc[504:1008].reset_index(),\n",
    "                           data.close.iloc[1008:1512].reset_index(),\n",
    "                           data.close.iloc[1512:2016].reset_index()],axis=1)\n",
    "    \n",
    "    \n",
    "    data_close = data_close.drop(\"None\",axis=1)\n",
    "    #qopen = np.array([q.get_open() for q in data]).astype(np.float)\n",
    "    #qclose = np.array([q.get_prev_close() for q in quotes]).astype(np.float)  \n",
    "    #qopen = np.array(data['open'],data['open']).astype(np.float)\n",
    "    #qclose = np.array(data['close'],data['close']).astype(np.float)  \n",
    "    qopen = np.array(data_open).astype(np.float)\n",
    "    qclose = np.array(data_close).astype(np.float)                    \n",
    "            \n",
    "    variation= qclose - qopen  #per day variation in price for each symbol\n",
    "    X = variation.T\n",
    "    X /= X.std(axis=0) #standardize to use correlations rather than covariance\n",
    "    #X = np.concatenate([X,X],axis=1)  \n",
    "#B = np.reshape(A, (-1, 2))          \n",
    "    #estimate inverse covariance  \n",
    "    X = X.T\n",
    "    graph = GraphicalLassoCV()\n",
    "    graph.fit(X)\n",
    "    \n",
    "    gl_cov = graph.covariance_\n",
    "    gl_prec = graph.precision_\n",
    "    gl_alphas =graph.cv_alphas_\n",
    "    gl_scores = np.mean(graph.grid_scores_, axis=1)\n",
    "\n",
    "    plt.figure()        \n",
    "    sns.heatmap(gl_prec)\n",
    "    \n",
    "    plt.figure()    \n",
    "    plt.plot(gl_alphas, gl_scores, marker='o', color='b', lw=2.0, label='GraphicalLassoCV')\n",
    "    plt.title(\"Graph Lasso Alpha Selection\")\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.legend()\n",
    "    \n",
    "    #cluster using affinity propagation\n",
    "    _, labels = cluster.affinity_propagation(gl_cov)\n",
    "    num_labels = np.max(labels)\n",
    "    \n",
    "    for i in range(num_labels+1):\n",
    "        print(\"Cluster %i: %s\" %((i+1), ', '.join(names[labels==i])))\n",
    "    \"\"\"\n",
    "    #find a low dim embedding for visualization\n",
    "    node_model = manifold.LocallyLinearEmbedding(n_components=2, n_neighbors=6, eigen_solver='dense')\n",
    "    embedding = node_model.fit_transform(X.T).T\n",
    "    \n",
    "    #generate plots\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    ax = plt.axes([0.,0.,1.,1.])\n",
    "    plt.axis('off')\n",
    "    \n",
    "    partial_corr = gl_prec\n",
    "    d = 1 / np.sqrt(np.diag(partial_corr))    \n",
    "    non_zero = (np.abs(np.triu(partial_corr, k=1)) > 0.02)  #connectivity matrix\n",
    "    \n",
    "    #plot the nodes\n",
    "    plt.scatter(embedding[0], embedding[1], s = 100*d**2, c = labels, cmap = plt.cm.spectral)\n",
    "    \n",
    "    #plot the edges\n",
    "    start_idx, end_idx = np.where(non_zero)\n",
    "    segments = [[embedding[:,start], embedding[:,stop]] for start, stop in zip(start_idx, end_idx)]\n",
    "    values = np.abs(partial_corr[non_zero])\n",
    "    lc = LineCollection(segments, zorder=0, cmap=plt.cm.hot_r, norm=plt.Normalize(0,0.7*values.max()))\n",
    "    lc.set_array(values)\n",
    "    lc.set_linewidths(5*values)\n",
    "    ax.add_collection(lc)\n",
    "    \n",
    "    #plot the labels\n",
    "    for index, (name, label, (x,y)) in enumerate(zip(names, labels, embedding.T)):\n",
    "        plt.text(x,y,name,size=12)\n",
    "\t\t\n",
    "\t\"\"\"\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\n",
    "\"\"\"****************************\n",
    "Gaussian Process Regression\n",
    "****************************\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas_datareader import data, wb\n",
    "\n",
    "#from sklearn.gaussian_process import GaussianProcess\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def f(x): return x * np.sin(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"639 main----------------------------------------------------1\")\n",
    "   \n",
    "    plt.close('all')\n",
    "   \n",
    "    #example: fit a GP (with noisy observations)\n",
    "    X = np.array([1., 3., 5., 6., 7., 8.]).reshape(-1,1)\n",
    "    y = f(X).ravel()\n",
    "    dy = 0.5 + 1.0*np.random.random(y.shape)  #in [0.5, 1.5] <- std deviation per point\n",
    "    y = y + np.random.normal(0, dy)  #0-mean noise with variable std in [0.5, 1.5]\n",
    "    gp = GaussianProcessRegressor(corr='cubic', nugget = (dy / y)**2, theta0=1e-1, thetaL=1e-3, thetaU=1, random_start=100, verbose=True)\n",
    "    gp.fit(X, y)  #ML est\n",
    "    gp.get_params()\n",
    "        \n",
    "    Xt = np.array(np.linspace(np.min(X)-10,np.max(X)+10,1000)).reshape(-1,1)\n",
    "    y_pred, MSE = gp.predict(Xt, eval_MSE=True)\n",
    "    sigma = np.sqrt(MSE)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(Xt, f(Xt), color='k', lw=2.0, label = 'x sin(x) ground truth')\n",
    "    plt.plot(X, y, 'r+', markersize=20, lw=2.0, label = 'observations')\n",
    "    plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label='Observations')    \n",
    "    plt.plot(Xt, y_pred, color = 'g', linestyle = '--', lw=1.5, label = 'GP prediction')\n",
    "    plt.fill(np.concatenate([Xt, Xt[::-1]]), np.concatenate([y_pred-1.96*sigma, (y_pred+1.96*sigma)[::-1]]), alpha = 0.5, label = '95% conf interval')\n",
    "    plt.title('GP regression')\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "            \n",
    "    #fit a GP to market data\n",
    "    #load data     \n",
    "    start = datetime(2015, 1, 1, 0, 0, 0, 0, pytz.utc)\n",
    "    end = datetime(2016, 1, 1, 0, 0, 0, 0, pytz.utc)    \n",
    "    spy = data.DataReader(\"SPY\", 'yahoo', start, end)\n",
    "    spy['Close'] = spy['Close'].dropna(axis=1,inplace=True)\n",
    "    spy['Volume'] = spy['Volume'].dropna(axis=1,inplace=True)\n",
    "    spy_price = np.array(spy['Close'].values).reshape(-1,1)\n",
    "    spy_volume = np.array(spy['Volume'].values).reshape(-1,1)\n",
    "    spy_obs = np.hstack([spy_price, spy_volume])\n",
    "      \n",
    "    #X = np.random.rand(np.size(spy_price)).reshape(-1,1)        \n",
    "    X = np.array(range(np.size(spy_price))).reshape(-1,1)\n",
    "    y = spy_price.ravel()\n",
    "    dy = 10*spy['Close'].std()\n",
    "    spy_gp = GaussianProcessRegressor(corr='cubic', nugget = (dy/y)**2, theta0=1e-1, thetaL=1e-3, thetaU=1e3, random_start=100, verbose=True)\n",
    "    \n",
    "    spy_gp.fit(X,y)\n",
    "    \n",
    "    spy_gp.get_params()\n",
    "        \n",
    "    Xt = np.array(np.linspace(np.min(X)-10,np.max(X)+10,1000)).reshape(-1,1)\n",
    "    y_pred, MSE = spy_gp.predict(Xt, eval_MSE=True)\n",
    "    sigma = np.sqrt(MSE)            \n",
    "\n",
    "    f = plt.figure()\n",
    "    plt.plot(X, y, 'r-', markersize=20, lw=2.0, label = 'SPY price, USD')\n",
    "    #plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label='Observations')    \n",
    "    plt.plot(Xt, y_pred, color = 'g', linestyle = '--', lw=1.5, label = 'GP prediction')\n",
    "    plt.fill(np.concatenate([Xt, Xt[::-1]]), np.concatenate([y_pred-1.96*sigma, (y_pred+1.96*sigma)[::-1]]), alpha = 0.5, label = '95% conf interval')\n",
    "    plt.title('GP regression')\n",
    "    plt.xlabel('time, days')\n",
    "    plt.ylabel('S&P500 price, USD')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cross_validation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-55accad14dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named cross_validation"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GaussianProcessClassifier()\n",
    "GaussianProcessRegressor()\n",
    "GaussianProcessRegressor(corr='cubic', nugget = (dy/y)**2, theta0=1e-1, thetaL=1e-3, thetaU=1e3, random_start=100, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
